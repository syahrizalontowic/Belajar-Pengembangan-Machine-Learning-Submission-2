# -*- coding: utf-8 -*-
"""Submission_2_NLP_Electricity_Consumption.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1JcfapoCoROVoClCRTpEoZNd_B3Oisa5j
"""

# Untuk mengupload dari drive lokal 
from google.colab import drive
drive.mount('/content/drive/')

!pip install -q keras

# Impor library yang di perlukan
import keras
import numpy as np
import pandas as pd
from keras.layers import Bidirectional
from keras.layers import Dense, LSTM, Dropout
import matplotlib.pyplot as plt
import tensorflow as tf

# Ubah dataset menjadi dataframe dengan fungsi read_csv(). 
# Tampilkan 5 data teratas pada dataframe menggunakan fungsi head().
data_train = pd.read_csv('/content/drive/My Drive/Colab Notebooks/Dataset/Electricity Consumption/train.csv')
data_train.head()

# Untuk menampilkan jumlah colomns dan data
data_train.info()

# Mengecek apakah ada nilai yang hilang dari dataset menggunakan fungsi isnull().
data_train.isnull().sum()

date = data_train['datetime']
temp = data_train['temperature']
print(temp.shape)
print(temp)

# Untuk membuat plot dari data kita dapat menggunakan fungsi plot dari library matplotlib.
# Output cell yang kita jalankan menunjukkan bahwa data kita merupakan time series yang bersifat penggunaan.
date = data_train['datetime'].values
temp = data_train['temperature'].values

plt.figure(figsize=(20,7))
plt.plot(date, temp)
plt.title('Temperature Average',
          fontsize=20);

# Split jadi train-test
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(date, temp, test_size=0.2)

# Konversi teks ke tfidf
from keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

tokenizer = Tokenizer()
# Fit hanya berdasarkan data train
tokenizer.fit_on_texts(X_train)
# Konversi train
seq_x_train = tokenizer.texts_to_sequences(X_train)
X_enc_train = tokenizer.sequences_to_matrix(seq_x_train,mode="tfidf")
# Konversi test
seq_x_test  = tokenizer.texts_to_sequences(X_test)
X_enc_test  = tokenizer.sequences_to_matrix(seq_x_test,mode="tfidf")
 
print(X_enc_train.shape)
print(X_enc_test.shape)
print(X_enc_train)

# Untuk merubah data kita menjadi format yang dapat diterima oleh model. 
# Fungsi di bawah menerima sebuah series/atribut kita yang telah di konversi menjadi tipe numpy, 
# Kemudian mengembalikan label dan atribut dari dataset dalam bentuk batch.
def windowed_dataset(series, window_size, batch_size, shuffle_buffer):
    series = tf.expand_dims(series, axis=-1)
    ds = tf.data.Dataset.from_tensor_slices(series)
    ds = ds.window(window_size + 1, shift=1, drop_remainder=True)
    ds = ds.flat_map(lambda w: w.batch(window_size + 1))
    ds = ds.shuffle(shuffle_buffer)
    ds = ds.map(lambda w: (w[:-1], w[-1:]))
    return ds.batch(batch_size).prefetch(1)

# Untuk arsitektur model gunakan 2 buah layer LSTM.
# Ketika menggunakan 2 buah layer LSTM, perhatikan bahwa layer pertama harus memiliki parameter return_sequences yang bernilai True.
train_set = windowed_dataset(temp, window_size=60, batch_size=100, shuffle_buffer=1000)
test_set = windowed_dataset(temp, window_size=60, batch_size=100, shuffle_buffer=1000)

model = tf.keras.models.Sequential([
    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(60, return_sequences=True)),
    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(60)),
    tf.keras.layers.Dropout(0.5),
    tf.keras.layers.Dropout(0.5),
    tf.keras.layers.Dense(120, activation='relu'),
    tf.keras.layers.Dense(60, activation='relu'),
    tf.keras.layers.Dense(30, activation='relu'),
    tf.keras.layers.Dense(1)
])

# Pada optimizer, kita akan menggunakan parameter learning rate dan momentum
# Loss function yang dapat dicoba untuk ini adalah Huber yang merupakan salah satu loss function yang umum digunakan pada kasus time series.
# Serta metrik yang digunakan untuk mengevaluasi model adalah MAE.
optimizer = tf.keras.optimizers.SGD(lr=1.0000e-03, momentum=0.9)
model.compile(loss=tf.keras.losses.Huber(),
              optimizer=optimizer,
              metrics=["mae"])

# Untuk mengetahui nilai data rata-rata, terkecil, dan terbesar pada colom AEP_MW
temp = np.array(data_train['temperature'])
print(temp)
print("temperature terkecil:    ", temp.min())
print("temperature terbesar:    ", temp.max())
print("temperature rata-rata:   ", temp.mean())

# Mengimplementasikan callback
class stopTraining(tf.keras.callbacks.Callback):
  def on_epoch_end(self, epoch, logs={}):
    if(logs.get('mae') is not None and logs.get('mae') <= 4.07):  
      # Do something else
      self.model.stop_training = True

custom_callback = stopTraining()

# Menggunakan 30 epoch. atau bebas bereksperimen dengan nilai yang lain
# Mulai melatih model kita dengan memanggil fungsi fit()
num_epochs = 30
history = model.fit(train_set, epochs=num_epochs, batch_size=2, steps_per_epoch=30, 
                    validation_data=test_set, callbacks=[custom_callback])

# Menguji akurasi prediksi model pada data uji val_loss & val_mae
results = model.evaluate(test_set,)
print("Hasil  [val_loss,val_mae] untuk data test:")
print(results)

# Membuat plot mae atau loss dari model. Kode di bawah menunjukkan bagaimana kita bisa membuat plot loss dan mae dari model
mae = history.history['mae']
val_mae = history.history['val_mae']
loss = history.history['loss']
val_loss = history.history['val_loss']

epochs_range = range(4)

plt.figure(figsize=(8, 8))
plt.subplot(1, 2, 1)
plt.plot(epochs_range, mae, 'r', label='Mae')
plt.plot(epochs_range, val_mae, 'b', label='Validation Mae')
plt.legend(loc='lower left')
plt.title('Mae and Validation Mae')

plt.subplot(1, 2, 2)
plt.plot(epochs_range, loss, 'r', label='Loss')
plt.plot(epochs_range, val_loss, 'b', label='Validation Loss')
plt.legend(loc='upper right')
plt.title('Loss and Validation Loss')
plt.show()